{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-97473568ca57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, val_dataset), dataset_info = tfds.load(\n",
    "    \"coco/2017\", split=[\"train\", \"validation\"], with_info=True, data_dir=\"data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_coco_to_corner(boxes):\n",
    "    print(boxes)\n",
    "    return tf.stack([\n",
    "        boxes[:,0], \n",
    "        boxes[:,2], \n",
    "        boxes[:,1], \n",
    "        boxes[:,3]\n",
    "    ], axis=-1)\n",
    "    \n",
    "\n",
    "\n",
    "def swap_xy(boxes):\n",
    "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
    "\n",
    "    Arguments:\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
    "\n",
    "    Returns:\n",
    "      swapped boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    \"\"\"Changes the box format to center, width and height.\n",
    "\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[xmin, ymin, xmax, ymax]`.\n",
    "\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.stack(\n",
    "        [(boxes[:,0] + boxes[:, 2]) / 2,\n",
    "        (boxes[:, 1] + boxes[:, 3]) / 2,\n",
    "        boxes[:, 2] - boxes[:,0], \n",
    "        boxes[:, 3] - boxes[:, 1]], \n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    \"\"\"Changes the box format to corner coordinates\n",
    "\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[x, y, width, height]`.\n",
    "\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "\n",
    "    Arguments:\n",
    "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "\n",
    "    Returns:\n",
    "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
    "        jth column holds the IOU between ith box and jth box from\n",
    "        boxes1 and boxes2 respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert boxes to corners to compute iou \n",
    "    box1 = convert_to_corners(boxes1) \n",
    "    box2 = convert_to_corners(boxes2) \n",
    "    \n",
    "    # for each gt bbox, compare it's (xmin,ymin) coords with each anchor \n",
    "    # comparing A with B with the following : A[:,None, xxxx] ; B[xxx,xx] \n",
    "    upper_left = tf.maximum(box1[:,None,:2], box2[:, :2]) \n",
    "    lower_down = tf.minimum(box1[:, None, 2:], box2[:, 2:]) \n",
    "    print(\"upper left\", upper_left.shape) \n",
    "    print(\"lower down\",lower_down.shape) \n",
    "    # if area of the intersection is < 0 => clip it to 0 \n",
    "    area_dims = tf.maximum(0.0, lower_down - upper_left)\n",
    "    intersection_area = upper_left[:,:,0] * lower_down[:,:,1] \n",
    "    print(\"intersection_area\", intersection_area.shape)\n",
    "    # computing union between boxes \n",
    "    box1_union = box1[:,2:] - box1[:,:2] # compute width/height for each box \n",
    "    box2_union = box2[:,2:] - box2[:,:2] # *** \n",
    "    print(\"box1 union\", box1_union.shape) \n",
    "    print(\"box2 union\", box2_union.shape)\n",
    "    # compute all boxes union \n",
    "    box1_union = box1_union[:,0] * box1_union[:,1] \n",
    "    box2_union = box2_union[:,0] * box2_union[:,1] \n",
    "    print(\"box1 union\", box1_union.shape) \n",
    "    print(\"box2 union\", box2_union.shape)\n",
    "    # computing the total union \n",
    "    union_area = box1_union[:,None] + box2_union - intersection_area    \n",
    "    print(\"union_area\", union_area.shape)\n",
    "    # return the intersection over union \n",
    "    \n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0) \n",
    "\n",
    "def visualize_detections(\n",
    "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
    "    ):\n",
    "    \"\"\"Visualize Detections\"\"\"\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for box, _cls, score in zip(boxes, classes, scores):\n",
    "        text = \"{}: {:.2f}\".format(_cls, score)\n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        ax.text(\n",
    "            x1,\n",
    "            y1,\n",
    "            text,\n",
    "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
    "            clip_box=ax.clipbox,\n",
    "            clip_on=True,\n",
    "        )\n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anchor: \n",
    "    \n",
    "    def __init__(self): \n",
    "        self.aspect_ratios =  [0.2, 1, 2] \n",
    "        self.sizes = [2**i for i in [0, 1/3, 2/3]]\n",
    "        self.num_anchors = len(self.aspect_ratios) * len(self.sizes) \n",
    "        self.areas = [x**2 for x in [32, 64, 128, 256, 512]]\n",
    "        self.strides = [2**l for l in range(3,8)]\n",
    "        self.dims = self._get_anchor_dims()\n",
    "        \n",
    "        \n",
    "    def _get_anchor_dims(self): \n",
    "        \n",
    "        all_dimensions = []\n",
    "        \n",
    "        # iterating over convs\n",
    "        for area in self.areas: \n",
    "            dimensions = []\n",
    "            # creating the 9 anchor boxes \n",
    "            for ratio in self.aspect_ratios: \n",
    "                    \n",
    "                anchor_height = tf.math.sqrt(area/ratio)\n",
    "                anchor_width = area / anchor_height \n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), \n",
    "                    [1,1,2]\n",
    "                )\n",
    "                for size in self.sizes: \n",
    "                    # scale down \n",
    "                    dimensions.append(dims*size)\n",
    "            # append 2X9 tensors\n",
    "            all_dimensions.append(tf.stack(dimensions, axis=-2))\n",
    "        return all_dimensions\n",
    "                    \n",
    "    \n",
    "    def _get_anchors(self, feature_height, feature_width, level): \n",
    "        \n",
    "        # meshgrid \n",
    "        rx = tf.range(feature_width, dtype=\"float32\") + 0.5\n",
    "        ry = tf.range(feature_height, dtype=\"float32\") + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx,ry), axis=-1) * self.strides[level-3] \n",
    "        centers = tf.expand_dims(centers, axis=-2) \n",
    "        centers = tf.tile(centers, [1,1,self.num_anchors,1])\n",
    "        dims = tf.tile(self.dims[level-3], [feature_height, feature_width, 1,1])\n",
    "        \n",
    "        anchors= tf.concat([centers, dims], axis=-1)\n",
    "        anchors = tf.reshape(anchors, (feature_height * feature_width * self.num_anchors,4))\n",
    "        \n",
    "        return anchors\n",
    "    \n",
    "    def get_anchors(self, image_height, image_width): \n",
    "        \"\"\"\n",
    "            get anchors for each convolution level/stride  on the image\n",
    "        \"\"\"\n",
    "        \n",
    "        anchors = [\n",
    "            self._get_anchors(tf.math.ceil(image_height/2**stride),\n",
    "                              tf.math.ceil(image_width/2**stride),\n",
    "                              stride\n",
    "            )\n",
    "            for stride in range(3,8)\n",
    "        ]\n",
    "        print(\"FINAL anchors shape\", tf.concat(anchors, axis=0))\n",
    "        return tf.concat(anchors, axis=0)\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anchor = Anchor()\n",
    "anchor._get_anchors(2, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(sample): \n",
    "    \"\"\"\n",
    "        preprocessing for a single sample \n",
    "    \"\"\"\n",
    "    \n",
    "    image = sample[\"image\"] \n",
    "    bbox = sample[\"objects\"][\"bbox\"] \n",
    "    cls_id = sample[\"objects\"][\"label\"]\n",
    "    \n",
    "    #scaling bbox coords\n",
    "    bbox = tf.stack([\n",
    "        bbox[:,0] * image.shape[1], \n",
    "        bbox[:,1] * image.shape[0], \n",
    "        bbox[:,2] * image.shape[1], \n",
    "        bbox[:,3] * image.shape[0]\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # convert box to xywh\n",
    "    bbox = convert_to_xywh(from_coco_to_corner(bbox)) \n",
    "    \n",
    "    return image, bbox, cls_id\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder: \n",
    "    \"\"\"\n",
    "        this class aims at encoding a raw sample into a training input and target \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self): \n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )\n",
    "    \n",
    "    def _match_anchors(self, bbox, anchors,match_iou=0.5, ignore_iou=0.4): \n",
    "        \"\"\"\n",
    "            matches ground truth bbox to anchors \n",
    "        \"\"\" \n",
    "        # compute the iou matrix\n",
    "        iou_matrix = compute_iou(anchors, bbox) \n",
    "        # we want to associate one bbox to an anchor \n",
    "        # if we have 3 bbox and 10 anchors, we want to have a (3, 10) (3 lines of 10 columns\n",
    "        # if the colmun value is 1, the bbox matches the actual anchor (anchor number : column number) \n",
    "        # then, we grab the corresponding anchor and build the target data \n",
    "        \n",
    "        print(\"iou matrix shape\",iou_matrix.shape)\n",
    "        print(\"iou matrix values\", iou_matrix) \n",
    "        # compute max iou for each row and consider all anchors with lower iou as background / ignored \n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1) \n",
    "        print(\"max_iou\", max_iou)\n",
    "        # get matching anchors \n",
    "        matched_anchors = tf.argmax(iou_matrix, axis=1) \n",
    "        # negative examples \n",
    "        negative_examples = tf.math.less(max_iou, match_iou)\n",
    "        print(\"neg\", negative_examples)\n",
    "        print(\"iou matrix\",iou_matrix) \n",
    "        print(\"max iou\", max_iou)\n",
    "        positive_examples = tf.math.greater_equal(max_iou, match_iou)\n",
    "        print(\"positive\",positive_examples)\n",
    "        ignored_examples = tf.math.logical_not(tf.math.logical_or(negative_examples, positive_examples))\n",
    "        print(\"ignored_examples\", ignored_examples)\n",
    "        return matched_anchors, tf.cast(positive_examples, dtype=\"float32\"), tf.cast(ignored_examples, dtype=\"float32\")\n",
    "    \n",
    "    def _compute_box_target(self, bbox, anchor): \n",
    "        \"\"\"\n",
    "            compute box target for training \n",
    "        \"\"\"\n",
    "        txy = (anchor[:,:2] - bbox[:,:2]) / anchor[:,2:]\n",
    "        twh = tf.math.log(anchor[:,2:] / bbox[:,2:])\n",
    "\n",
    "        target_bbox = tf.concat([txy, twh], axis=-1)\n",
    "        return target_bbox \n",
    "\n",
    "\n",
    "    def _encode_sample(self, image_shape, bbox, class_ids):\n",
    "        # generate anchor boxes for a given \n",
    "        anchors = Anchor().get_anchors(image_shape[0], image_shape[1])\n",
    "        # compute iou \n",
    "        matched_anchors_idx, positive_examples, ignored_examples = self._match_anchors(bbox, anchors)\n",
    "        print(\"matched_anchors\", matched_anchors_idx.shape) # each column is associated with a gtbox\n",
    "        matched_gt_box = tf.gather(bbox, matched_anchors_idx) # array of gt boxes \n",
    "        #compute targets \n",
    "        box_target = self._compute_box_target(matched_gt_box, anchors)\n",
    "        # associate labels \n",
    "        # get all labels \n",
    "        matched_gt_class_ids = tf.gather(class_ids, matched_anchors_idx)\n",
    "        print(\"matched gt class\", matched_gt_class_ids.shape) # each column is a class id \n",
    "        print(matched_gt_class_ids)\n",
    "        class_target = tf.where(\n",
    "            tf.math.not_equal(positive_examples, 1.0), \n",
    "            -1, \n",
    "            matched_gt_class_ids\n",
    "        )\n",
    "        class_target = tf.where(\n",
    "            tf.math.equal(ignored_examples, 1.0), \n",
    "            -2, \n",
    "            class_target\n",
    "        )\n",
    "        class_target = tf.expand_dims(class_target, axis=-1)\n",
    "        print(\"unique label value\",np.unique(class_target))\n",
    "        print(\"matched gt class\", class_target.shape) # each column is a class id \n",
    "        print(\"bbox target\", box_target.shape)\n",
    "        label = tf.concat([box_target, tf.cast(class_target, dtype=\"float32\")],axis=-1)\n",
    "        print(\"label\", label.shape) \n",
    "        \n",
    "        return label\n",
    "    \n",
    "    def encode_batch(self, batch_images, gt_boxes, gt_ids): \n",
    "        \"\"\"\n",
    "            create targets from a batch of images\n",
    "        \"\"\"\n",
    "        \n",
    "        labels = tf.TensorArray(dynamic_size=True)\n",
    "    \n",
    "        for idx in range(batch_images.shape[0]): \n",
    "            \n",
    "            # grab image, gt_boxes, gt_ids \n",
    "            image = batch_images[0, :,:]\n",
    "            label = self._encode_sample(image.shape, bbox[idx], gt_idx[idx]) \n",
    "            labels = labels.write(idx, label) \n",
    "            image = tf.keras.applications.resnet50.preprocess_input(image)\n",
    "        print(\"FINAL LABEL STACK\", labels.stack().shape)\n",
    "        return batch_images, labels.stack()\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_test(): \n",
    "    data = train_dataset.take(1)\n",
    "\n",
    "    data = list(data.as_numpy_iterator())[0]\n",
    "    image = data[\"image\"] \n",
    "    height, width = image.shape[:2]\n",
    "    bbox = swap_xy(data[\"objects\"][\"bbox\"])\n",
    "    # scaling the bbox to the image \n",
    "    bbox = tf.stack([bbox[:,0] * width, bbox[:,1] * height, (bbox[:,2] - bbox[:, 0]) * width, (bbox[:,3] - bbox[:,1]) * height], axis=-1)\n",
    "    cls = data[\"objects\"][\"label\"] \n",
    "\n",
    "    # anchor \n",
    "    anchor = Anchor()\n",
    "    image_anchors = anchor.get_anchors(width,height)[:10]\n",
    "    iou_matrix = compute_iou(bbox, image_anchors)\n",
    "    fig, ax = plt.subplots(figsize=(12,10))\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder._encode_sample((height, width), bbox, cls)\n",
    "    \n",
    "    print() \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(image_anchors.shape[0]):\n",
    "        anc = image_anchors[i,:]\n",
    "        rect = patches.Rectangle((anc[0], anc[1]), anc[2], anc[3], linewidth=1, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(rect)    \n",
    "        if i == 8:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    for box in bbox: \n",
    "        \n",
    "        rect = patches.Rectangle((box[0], box[1]), box[2], box[3], linewidth=4, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    print(\"IMAGE SHAPE\", image.shape)\n",
    "    plt.imshow(image) \n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone(nb_classes): \n",
    "    \"\"\"\n",
    "        load resnet50 and extract P3 to P7 convolution layers\n",
    "    \"\"\"\n",
    "    \n",
    "    backbone = tf.keras.applications.ResNet50(\n",
    "        include_top=False, \n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(224, 224, 3),\n",
    "        classes=nb_classes\n",
    "    )\n",
    "    \n",
    "    backbone.summary()\n",
    "    \n",
    "    # store output\n",
    "    c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output \n",
    "        for layer_name in [\"conv3_block4_out\", \"conv4_block4_out\", \"conv5_block3_out\"]\n",
    "    ]\n",
    "    return tf.keras.Model(inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_backbone(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturePyramidLayer(tf.keras.layers.Layer): \n",
    "\n",
    "    def __init__(self, nb_classes): \n",
    "        super(FeaturePyramidLayer, self).__init__()\n",
    "        self.backbone = get_backbone(nb_classes)\n",
    "        self.c3_1x1 = tf.keras.layers.Conv2D(256, 1, 1, padding=\"same\") \n",
    "        self.c4_1x1 = tf.keras.layers.Conv2D(256, 1, 1, padding=\"same\") \n",
    "        self.c5_1x1 = tf.keras.layers.Conv2D(256, 1, 1, padding=\"same\") \n",
    "        self.c3_3x3 = tf.keras.layers.Conv2D(256, 3, 1, padding=\"same\") \n",
    "        self.c4_3x3 = tf.keras.layers.Conv2D(256, 3, 1, padding=\"same\") \n",
    "        self.c5_3x3 = tf.keras.layers.Conv2D(256, 3, 1, padding=\"same\") \n",
    "        self.upsampling_layer = tf.keras.layers.UpSampling2D(2)\n",
    "        \n",
    "    def call(self, image, training=False): \n",
    "        c3_output, c4_output, c5_output = self.backbone(image, training=False)\n",
    "        p4_1x1 = self.c4_1x1(c4_output)\n",
    "        p3_1x1 = self.c3_1x1(c3_output)\n",
    "        \n",
    "        p5_out_pre = self.c5_1x1(c5_output)\n",
    "        p4_out_pre = self.upsampling_layer(p5_out_pre) + p4_1x1 \n",
    "        p3_out_pre = self.upsampling_layer(p4_out_pre) + p3_1x1\n",
    "        \n",
    "        p4_out = self.c4_3x3(p4_out_pre)\n",
    "        p3_out = self.c3_3x3(p3_out_pre) \n",
    "        p5_out = self.c5_3x3(p5_out_pre)\n",
    "        p6_out = tf.keras.layers.Conv2D(256, 3, 2)(c5_output)\n",
    "        p7_out = tf.keras.layers.Conv2D(256, 3, 2, activation=\"relu\")(p6_out)\n",
    "        return p3_out, p4_out, p5_out, p6_out, p7_out\n",
    "    \n",
    "    \n",
    "def build_head(output_filters): \n",
    "    \n",
    "    inputs = tf.keras.Input(shape=[None, None, 256])\n",
    "    x = inputs\n",
    "    for i in range(4): \n",
    "        x = tf.keras.layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(x) \n",
    "        \n",
    "    # final layer \n",
    "    x = tf.keras.layers.Conv2D(output_filters, 3, padding=\"same\")(x) \n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs], outputs=[x])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retinanet(tf.keras.Model): \n",
    "    \n",
    "    def __init__(self, nb_classes): \n",
    "        super(Retinanet, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.fpn =  FeaturePyramidLayer(self.nb_classes)\n",
    "        self.regression_head = build_head(9*4)\n",
    "        self.clf_head = build_head(9*self.nb_classes)\n",
    "        \n",
    "    def call(self, image): \n",
    "        # custom forward pass \n",
    "        # use the fpn network \n",
    "        N = tf.shape(image)[0]\n",
    "        print(\"N is \", N)\n",
    "        features = self.fpn(image)\n",
    "        regr_out, clf_out = [], [] \n",
    "        for feature in features:\n",
    "            print()\n",
    "            print(\"Previous regr_out\", self.regression_head(feature))\n",
    "            print(\"Adding to regr_out\", tf.reshape(self.regression_head(feature), [N,-1,4]))\n",
    "            regr_out.append(\n",
    "                tf.reshape(self.regression_head(feature), [N,-1,4])\n",
    "            )\n",
    "            \n",
    "            clf_out.append(\n",
    "                tf.reshape(self.clf_head(feature), [N,-1,self.nb_classes])\n",
    "            )\n",
    "            \n",
    "        \n",
    "        regr_out = tf.concat(regr_out, axis=1) \n",
    "        print(\"final regr_out\", regr_out)\n",
    "        print()\n",
    "        clf_out = tf.concat(clf_out, axis=1)\n",
    "        print(\"clf_out\", clf_out)\n",
    "            \n",
    "        return tf.concat([clf_out, regr_out], axis=-1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the retinanet call layer \n",
    "\n",
    "inputs = tf.keras.Input(shape=[224,224,3], batch_size=28) \n",
    "outputs = Retinanet(80).call(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeLayer(tf.keras.layers.Layer): \n",
    "    \n",
    "    def __init__(self): \n",
    "        pass \n",
    "    \n",
    "    \n",
    "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
    "        boxes = box_predictions * self._box_variance\n",
    "        boxes = tf.concat(\n",
    "            [\n",
    "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        boxes_transformed = convert_to_corners(boxes)\n",
    "        return boxes_transformed\n",
    "\n",
    "    \n",
    "    def call(self, images, predictions): \n",
    "        anchors = Anchor().get_anchors(image.shape[0], image.shape[1])\n",
    "        bbox_predictions = predictions[:,:,:4] \n",
    "        clf_predictions = tf.keras.activations.sigmoid(predictions[:,:,4:])\n",
    "        # decode the box \n",
    "        bbox_predictions = self._decode_box_predictions(anchors, bbox_predictions)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothL1Loss(tf.keras.losses.Loss): \n",
    "    \n",
    "    def __init__(self): \n",
    "        super(SmoothL1Loss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
    "        )\n",
    "        \n",
    "    def call(self, y_true, y_pred): \n",
    "        difference = y_pred - y_true\n",
    "        absolute_difference =  tf.math.abs(difference)\n",
    "        squared_difference = difference **2\n",
    "        \n",
    "        loss = tf.where(\n",
    "            absolute_difference < 1, \n",
    "            0.5*squared_difference, \n",
    "            absolute_difference - 0.5\n",
    "        )\n",
    "        \n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "class FocalLoss(tf.keras.losses.Loss): \n",
    "    \n",
    "    def __init__(self, gamma, alpha): \n",
    "        super(FocalLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetClfLoss\"\n",
    "        )\n",
    "        self.gamma = gamma \n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def call(self, y_true, y_pred): \n",
    "        \n",
    "        preds =  tf.keras.activations.sigmoid()(y_pred)\n",
    "        props = tf.keras.losses.CategoricalCrossentropy()(y_true,preds)\n",
    "        pt = tf.where(tf.math.not_equal(y_true, 1.0), 1-probs, probs)\n",
    "        loss = self.alpha * tf.pow((1-pt), self.gamma) * tf.math.log(pt) \n",
    "        return tf.reduce_sum(loss, axis=1)\n",
    "    \n",
    "    \n",
    "class RetinaNetLoss(tf.keras.losses.Loss): \n",
    "    \n",
    "    def __init__(self, nb_classes, alpha=0.25, gamma=2.0):\n",
    "        super(RetinaNetLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetLoss\"\n",
    "        )\n",
    "        self.nb_classes = nb_classes \n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma \n",
    "        self.regr_loss = SmoothL1Loss()\n",
    "        self.focal_loss = FocalLoss(gamma, alpha)\n",
    "\n",
    "    def call(self, y_true, y_pred): \n",
    "        \n",
    "        true_bbox = y_true[:,:,:4] \n",
    "        pred_bbox = y_pred[:,:,:4] \n",
    "        \n",
    "        true_clf = tf.one_hot(\n",
    "            y_true[:,:,4], \n",
    "            depth=self.nb_classes\n",
    "        )\n",
    "        \n",
    "        pred_clf = y_pred[:,:,4:]\n",
    "        \n",
    "        positive_masks = tf.cast(tf.math.greater(pred_clf, -1.0), dtype=\"int32\")\n",
    "        ignore_masks = tf.cast(tf.math.equal(pred_clf, -2.0), dtype=\"int32\") \n",
    "        \n",
    "        # box loss \n",
    "        box_loss = self.regr_loss(true_bbox, pred_bbox) \n",
    "        focal_loss = self.focal_loss(true_clf, pred_clf)\n",
    "        \n",
    "        # if negative mask, box loss = 0 \n",
    "        focal_loss = tf.where(tf.math.equal(ignore_masks,1.0), 0, box_loss)\n",
    "        box_loss = tf.where(tf.math.not_equal(positive_masks, 1.0), box_loss, 0.0) \n",
    "        \n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        loss = clf_loss + box_loss\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"retinanet_model_dir/\"\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "num_classes = 80\n",
    "batch_size = 1\n",
    "\n",
    "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=learning_rate_boundaries, values=learning_rates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_backbone = get_backbone(num_classes)\n",
    "loss_fn = RetinaNetLoss(num_classes)\n",
    "model = Retinanet(num_classes)\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
    "model.compile(loss=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
